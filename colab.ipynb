{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "honAiTATzExA"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2zIFf7XX7fx"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4KHRsoaYFW3"
      },
      "outputs": [],
      "source": [
        "!pip install -q numpy pandas librosa soundfile xgboost lightgbm scikit-learn scipy tqdm rich"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz-3HkAaZIrZ"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDEQclGsYVQ1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, Subset, random_split\n",
        "from torch import optim\n",
        "from torch.optim.swa_utils import AveragedModel, SWALR, update_bn\n",
        "from torch.optim.lr_scheduler import OneCycleLR, ReduceLROnPlateau, CosineAnnealingLR, SequentialLR, LinearLR\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from torch.utils.tensorboard.writer import SummaryWriter\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import scipy, scipy.signal, scipy.stats\n",
        "import librosa\n",
        "import soundfile\n",
        "import os, re, math, warnings, random\n",
        "from rich.console import Console\n",
        "# from tqdm import tqdm # Î©ÄÌã∞Ïì∞Î†àÎî© Í∞ÄÎÅî Î¨∏Ï†úÏûáÏùå ÏïàÏì∞ÎäîÍ±∏Î°ú\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0GyQ5o7yiEp"
      },
      "outputs": [],
      "source": [
        "train_csv_path = \"/content/drive/MyDrive/ST_KAGGLE_2/input/train.csv\"\n",
        "test_csv_path = \"/content/drive/MyDrive/ST_KAGGLE_2/input/test.csv\"\n",
        "train_data_path = \"/content/drive/MyDrive/ST_KAGGLE_2/input/train\"\n",
        "test_data_path = \"/content/drive/MyDrive/ST_KAGGLE_2/input/test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMhC4a9Rv5U3"
      },
      "outputs": [],
      "source": [
        "# Íµ¨Í∏ÄÎìúÎùºÏù¥Î∏åÎ°ú Ìä∏Î†àÏù¥ÎãùÌïòÎ©¥ ÏóÑÏ≤≠ Ïò§ÎûòÍ±∏Î¶º!!! Îü∞ÌÉÄÏûÑ Ï≤òÏùå ÏãúÏûëÌïòÎ©¥ Ïπ¥Ìîº Ìïú Î≤àÎßå Ìï¥ÎëêÍ∏∞\n",
        "# !unzip -q /content/drive/MyDrive/ST_KAGGLE_2/input/precomputed.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dqvG0LIwDKA"
      },
      "outputs": [],
      "source": [
        "precomputed_dir = \"/content/precomputed\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sVy4B_Q0SDb"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # A100\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.deterministic = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO66U2geb41M"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqkp5wpJzt7r"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppFbRw0OzuuR"
      },
      "outputs": [],
      "source": [
        "console = Console()\n",
        "\n",
        "def print_start(title: str):\n",
        "    console.print(f\"[bold cyan]üê∂ ÏûëÏóÖ ÏãúÏûë: {title}[/bold cyan]\")\n",
        "\n",
        "def print_epoch_summary(epoch_index: int, average_loss: float):\n",
        "    console.print(f\"[bold blue]‚öôÔ∏è Epoch {epoch_index} Summary[/bold blue]\")\n",
        "    console.print(f\"[green]ÌèâÍ∑† Training Loss:[/green] {average_loss:.4f}\")\n",
        "\n",
        "def print_validation_accuracy(accuracy: float, min_prob: float, max_prob: float):\n",
        "    console.print(f\"[bold green]‚úÖ Val Accuracy:[/bold green] {accuracy:.4f}\")\n",
        "    console.print(f\"[dim]Probability range: {min_prob:.3f}‚Äì{max_prob:.3f}[/dim]\")\n",
        "\n",
        "def progress_bar(iterable, description: str):\n",
        "    return tqdm(iterable, desc=description, ncols=120)\n",
        "\n",
        "def print_success(message: str):\n",
        "    console.print(f\"[bold green]‚úÖ {message}[/bold green]\")\n",
        "\n",
        "def print_warning(message: str):\n",
        "    console.print(f\"[bold yellow]‚ö†Ô∏è {message}[/bold yellow]\")\n",
        "\n",
        "def print_error(message: str):\n",
        "    console.print(f\"[bold red]‚ùå {message}[/bold red]\")\n",
        "\n",
        "def print_info(message: str):\n",
        "    console.print(f\"[bold blue]‚ÑπÔ∏è {message}[/bold blue]\")\n",
        "\n",
        "def count_parameters(model):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    console.print(f\"[bold purple]ü¶Ñ Ï¥ù ÌååÎùºÎØ∏ÌÑ∞: {total:,}[/bold purple]\")\n",
        "    console.print(f\"[bold purple]ü¶Ñ Trainable: {trainable:,}[/bold purple]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WToQ68s2hDpG"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X-ydEmqhEl5"
      },
      "outputs": [],
      "source": [
        "# with soundfile.SoundFile(train_data_path+\"/steth_20190623_10_35_17_033.wav\") as audio:\n",
        "#     waveform_i = audio.read(dtype=\"float32\")\n",
        "#     sr = audio.samplerate\n",
        "#     plt.figure(figsize=(15, 4))\n",
        "#     plt.subplot(1,2,1)\n",
        "#     librosa.display.waveshow(waveform_i, sr=sr)\n",
        "#     plt.title('Inhale')\n",
        "\n",
        "# with soundfile.SoundFile(train_data_path+\"/steth_20190623_10_34_54_018.wav\") as audio:\n",
        "#     waveform_e = audio.read(dtype=\"float32\")\n",
        "#     sr = audio.samplerate\n",
        "#     plt.subplot(1,2,2)\n",
        "#     librosa.display.waveshow(waveform_e, sr=sr)\n",
        "#     plt.title('Exhale')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_v4OU0ffij6z"
      },
      "outputs": [],
      "source": [
        "# stft_spectrum_matrix = librosa.stft(waveform_i)\n",
        "# plt.figure(figsize=(10, 4))\n",
        "# librosa.display.specshow(librosa.amplitude_to_db(np.abs(stft_spectrum_matrix), ref=np.max),y_axis='log', x_axis='time')\n",
        "# plt.title('Inhale STFT Power spectrogram')\n",
        "# plt.colorbar(format='%+2.0f dB')\n",
        "# plt.tight_layout()\n",
        "\n",
        "# stft_spectrum_matrix = librosa.stft(waveform_e)\n",
        "# plt.figure(figsize=(10, 4))\n",
        "# librosa.display.specshow(librosa.amplitude_to_db(np.abs(stft_spectrum_matrix), ref=np.max),y_axis='log', x_axis='time')\n",
        "# plt.title('Exhale STFT Power spectrogram')\n",
        "# plt.colorbar(format='%+2.0f dB')\n",
        "# plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US6vzh6bkaXm"
      },
      "outputs": [],
      "source": [
        "# from matplotlib.colors import Normalize\n",
        "# mfc_coefficients = librosa.feature.mfcc(y=waveform_i, sr=sr, n_mfcc=32)\n",
        "# plt.figure(figsize=(10, 4))\n",
        "# librosa.display.specshow(mfc_coefficients, x_axis='time',norm=Normalize(vmin=-30,vmax=30))\n",
        "# plt.colorbar()\n",
        "# plt.yticks(())\n",
        "# plt.ylabel('MFC Coefficient')\n",
        "# plt.title('Inhale MFC Coefficients')\n",
        "# plt.tight_layout()\n",
        "\n",
        "# mfc_coefficients = librosa.feature.mfcc(y=waveform_e, sr=sr, n_mfcc=32)\n",
        "# plt.figure(figsize=(10, 4))\n",
        "# librosa.display.specshow(mfc_coefficients, x_axis='time',norm=Normalize(vmin=-30,vmax=30))\n",
        "# plt.colorbar()\n",
        "# plt.yticks(())\n",
        "# plt.ylabel('MFC Coefficient')\n",
        "# plt.title('Exhale MFC Coefficients')\n",
        "# plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWZO_rtekxce"
      },
      "outputs": [],
      "source": [
        "# melspectrogram_i = librosa.feature.melspectrogram(y=waveform_i, sr=sr, n_mels=128, fmax=4000)\n",
        "# plt.figure(figsize=(10, 4))\n",
        "# librosa.display.specshow(librosa.power_to_db(S=melspectrogram_i, ref=np.mean),y_axis='mel',fmax=4000, x_axis='time', norm=Normalize(vmin=-20,vmax=20))\n",
        "# plt.colorbar(format='%+2.0f dB',label='Amplitude')\n",
        "# plt.ylabel('Mels')\n",
        "# plt.title('Inhale Mel spectrogram')\n",
        "# plt.tight_layout()\n",
        "\n",
        "# melspectrogram_e = librosa.feature.melspectrogram(y=waveform_e, sr=sr, n_mels=128, fmax=4000)\n",
        "# plt.figure(figsize=(10, 4))\n",
        "# librosa.display.specshow(librosa.power_to_db(S=melspectrogram_e, ref=np.mean),y_axis='mel',fmax=4000, x_axis='time', norm=Normalize(vmin=-20,vmax=20))\n",
        "# plt.colorbar(format='%+2.0f dB',label='Amplitude')\n",
        "# plt.ylabel('Mels')\n",
        "# plt.title('Exhale Mel spectrogram')\n",
        "# plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLWszlrelZZ0"
      },
      "outputs": [],
      "source": [
        "# chromagram = librosa.feature.chroma_stft(y=waveform_i, sr=sr)\n",
        "# plt.figure(figsize=(10, 4))\n",
        "# librosa.display.specshow(chromagram, y_axis='chroma', x_axis='time')\n",
        "# plt.colorbar(label='Relative Intensity')\n",
        "# plt.title('Inhale Chromagram')\n",
        "# plt.tight_layout()\n",
        "\n",
        "# chromagram = librosa.feature.chroma_stft(y=waveform_e, sr=sr)\n",
        "# plt.figure(figsize=(10, 4))\n",
        "# librosa.display.specshow(chromagram, y_axis='chroma', x_axis='time')\n",
        "# plt.colorbar(label='Relative Intensity')\n",
        "# plt.title('Exhale Chromagram')\n",
        "# plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIVDCzdPlkd_"
      },
      "outputs": [],
      "source": [
        "# def feature_chromagram(waveform, sample_rate):\n",
        "#     stft_spectrogram=np.abs(librosa.stft(waveform))\n",
        "#     chromagram=np.mean(librosa.feature.chroma_stft(S=stft_spectrogram, sr=sample_rate).T,axis=0)\n",
        "#     return chromagram\n",
        "\n",
        "# def feature_melspectrogram(waveform, sample_rate):\n",
        "#     melspectrogram=np.mean(librosa.feature.melspectrogram(y=waveform, sr=sample_rate, n_mels=128, fmax=4000).T,axis=0)\n",
        "#     return melspectrogram\n",
        "\n",
        "# def feature_mfcc(waveform, sample_rate):\n",
        "#     mfc_coefficients=np.mean(librosa.feature.mfcc(y=waveform, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
        "#     return mfc_coefficients\n",
        "\n",
        "# def get_features(file):\n",
        "#     with soundfile.SoundFile(file) as audio:\n",
        "#         waveform = audio.read(dtype=\"float32\")\n",
        "#         sample_rate = audio.samplerate\n",
        "\n",
        "#     chromagram = feature_chromagram(waveform, sample_rate) # 12,\n",
        "#     melspectrogram = feature_melspectrogram(waveform, sample_rate) # 128,\n",
        "#     mfc_coefficients = feature_mfcc(waveform, sample_rate) # 40,\n",
        "\n",
        "#     feature_vector = np.hstack((chromagram, melspectrogram, mfc_coefficients)).astype(np.float32)\n",
        "\n",
        "#     return feature_vector # 12 + 128 + 40 = 180,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdRegnDZmOVJ"
      },
      "outputs": [],
      "source": [
        "# inhale_matrix = get_features(train_data_path+\"/steth_20190623_10_35_17_033.wav\")\n",
        "# exhale_matrix = get_features(train_data_path+\"/steth_20190623_10_34_54_018.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8nNA20Gx6mN"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WV6m3S1CYXXy"
      },
      "outputs": [],
      "source": [
        "class DS(Dataset):\n",
        "    EXCLUDED_KEYS = {'scalars', 'sr', 'hop_length', 'n_fft'}\n",
        "\n",
        "    def __init__(self, data_frame: pd.DataFrame, feature_dir: str, is_training: bool):\n",
        "        self.df = data_frame.reset_index(drop=True)\n",
        "        self.feature_dir = feature_dir\n",
        "        self.is_training = is_training\n",
        "\n",
        "        self._detect_features()\n",
        "\n",
        "    def _detect_features(self):\n",
        "        if len(self.df) == 0:\n",
        "            raise ValueError\n",
        "\n",
        "        first_id = self.df.iloc[0][\"ID\"]\n",
        "        npz_path = os.path.join(self.feature_dir, first_id + \".npz\")\n",
        "\n",
        "        with np.load(npz_path) as data:\n",
        "            self.feature_names = [k for k in data.keys() if k not in self.EXCLUDED_KEYS]\n",
        "            self.feature_names.sort()\n",
        "\n",
        "            self.n_features = len(self.feature_names)\n",
        "            self.scalar_dim = data['scalars'].shape[0]\n",
        "\n",
        "        print(f\"#Features: {self.n_features} - {', '.join(self.feature_names)}\")\n",
        "        print(f\"#Scalars: {self.scalar_dim}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        file_id = row[\"ID\"]\n",
        "        npz_path = os.path.join(self.feature_dir, file_id + \".npz\")\n",
        "\n",
        "        data = np.load(npz_path)\n",
        "\n",
        "        features_list = []\n",
        "        for feat_name in self.feature_names:\n",
        "            features_list.append(data[feat_name])\n",
        "\n",
        "        features = np.stack(features_list, axis=0).astype(np.float32)\n",
        "        features = torch.from_numpy(features)\n",
        "\n",
        "        scalars = torch.from_numpy(data['scalars'].astype(np.float32))\n",
        "\n",
        "        if self.is_training:\n",
        "            label = 1.0 if row[\"Target\"] == \"E\" else 0.0\n",
        "            return features, scalars, torch.tensor(label, dtype=torch.float32)\n",
        "        else:\n",
        "            return features, scalars, file_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwuWVysKerbM"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    feats, scals, labs_or_ids = [], [], []\n",
        "    for f, s, y in batch:\n",
        "        feats.append(f)\n",
        "        scals.append(s)\n",
        "        labs_or_ids.append(y)\n",
        "\n",
        "    features = torch.stack(feats, dim=0)\n",
        "    scalars  = torch.stack(scals, dim=0) # Ïù¥Í±∞ Í∞úÏàòÎåÄÎ°ú Î™®Îç∏Ïóê num scalar features ÏßÄÏ†ïÌïòÍ∏∞\n",
        "\n",
        "    if isinstance(labs_or_ids[0], torch.Tensor):\n",
        "        labels = torch.stack(labs_or_ids, dim=0)\n",
        "        return features, scalars, labels\n",
        "    else:\n",
        "        return features, scalars, labs_or_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3ZQUx16ErI6"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsd1u8uShDMO"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.75, gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        p = torch.sigmoid(logits)\n",
        "        pt = torch.where(targets == 1, p, 1 - p)\n",
        "        weight = self.alpha * (1 - pt) ** self.gamma\n",
        "        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
        "        return (weight * bce).mean()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        reduced_channels = max(channels // reduction, 16)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, reduced_channels, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(reduced_channels, channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)"
      ],
      "metadata": {
        "id": "CK6nPmL4fnnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAgbytTZEtOf"
      },
      "source": [
        "## VGG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tgiUhdFoPEe"
      },
      "outputs": [],
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self, in_channels=9, num_scalar_features=39, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(64, 64, 3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(64, 64, 3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout2d(dropout_rate * 0.5)\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(128, 128, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(128, 128, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.GELU(),\n",
        "            nn.MaxPool2d(2, 2, ceil_mode=True),\n",
        "            nn.Dropout2d(dropout_rate)\n",
        "        )\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.GELU(),\n",
        "            nn.MaxPool2d(2, 2, ceil_mode=True),\n",
        "            nn.Dropout2d(dropout_rate)\n",
        "        )\n",
        "\n",
        "        self.block4_conv = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(512, 512, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(512, 512, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout2d(dropout_rate)\n",
        "        )\n",
        "\n",
        "        self.block4_residual = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 1, bias=False),\n",
        "            nn.BatchNorm2d(512)\n",
        "        )\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.scalar_net = nn.Sequential(\n",
        "            nn.Linear(num_scalar_features, 64, bias=False),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(64, 64, bias=False),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 + 64, 256, bias=False),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(256, 128, bias=False),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, features, scalars):\n",
        "        x = self.block1(features)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        residual = self.block4_residual(x)\n",
        "        x = self.block4_conv(x) + residual\n",
        "        x = self.global_pool(x).view(x.size(0), -1)\n",
        "        s = self.scalar_net(scalars)\n",
        "        combined = torch.cat([x, s], dim=1)\n",
        "        return self.classifier(combined).squeeze(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAeQHS81VJeA"
      },
      "source": [
        "## CNN 8 Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvv7esvNVJSI"
      },
      "outputs": [],
      "source": [
        "class CNN8(nn.Module):\n",
        "    def __init__(self, in_channels=9, num_scalar_features=39, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout2d(dropout_rate),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        self.scalar_net = nn.Sequential(\n",
        "            nn.Linear(num_scalar_features, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 + 64, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, features, scalars):\n",
        "        x = self.cnn(features)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        s = self.scalar_net(scalars)\n",
        "        combined = torch.cat([x, s], dim=1)\n",
        "        return self.classifier(combined).squeeze(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR_2YQzBE4xB"
      },
      "source": [
        "# Train and Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b-5wwUTxAXe"
      },
      "outputs": [],
      "source": [
        "SR = 16000\n",
        "DURATION = 1.0\n",
        "EXPECTED_LEN = int(SR * DURATION)\n",
        "N_MELS = 128\n",
        "N_MFCC = 40\n",
        "HOP_LENGTH = 256\n",
        "N_FFT = 512\n",
        "FMAX = 4500\n",
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = 4\n",
        "NUM_EPOCHS = 30\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-4\n",
        "SWA_START_RATIO = 0.75\n",
        "SEED_LIST = [0, 1, 2]\n",
        "T_FIXED = (EXPECTED_LEN // HOP_LENGTH) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTWGhun1w96Q"
      },
      "source": [
        "## Train Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzJkk3EtT4_U"
      },
      "source": [
        "## Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWvbELyRT6v0"
      },
      "outputs": [],
      "source": [
        "def cutmix_data(features, labels, alpha=1.0, device='cuda'):\n",
        "    batch_size = features.size(0)\n",
        "    indices = torch.randperm(batch_size).to(device)\n",
        "\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "\n",
        "    W = features.size(3)\n",
        "    H = features.size(2)\n",
        "\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = np.int32(W * cut_rat)\n",
        "    cut_h = np.int32(H * cut_rat)\n",
        "\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    features_mixed = features.clone()\n",
        "    features_mixed[:, :, bby1:bby2, bbx1:bbx2] = features[indices, :, bby1:bby2, bbx1:bbx2]\n",
        "\n",
        "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n",
        "\n",
        "    labels_mixed = lam * labels + (1 - lam) * labels[indices]\n",
        "\n",
        "    return features_mixed, labels_mixed, indices, lam\n",
        "\n",
        "\n",
        "def mixup_data(features, labels, alpha=1.0, device='cuda'):\n",
        "    batch_size = features.size(0)\n",
        "    indices = torch.randperm(batch_size).to(device)\n",
        "\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "\n",
        "    mixed_features = lam * features + (1 - lam) * features[indices]\n",
        "    mixed_labels = lam * labels + (1 - lam) * labels[indices]\n",
        "\n",
        "    return mixed_features, mixed_labels, indices, lam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duEY6nGUT8C_"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dE13VSn7oRbp"
      },
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    device: torch.device,\n",
        "    save_dir: str,\n",
        "    num_epochs: int = 30,\n",
        "    base_lr: float = 1e-3,\n",
        "    weight_decay: float = 1e-4,\n",
        "    patience: int = 15,\n",
        "    min_delta: float = 1e-4,\n",
        "    monitor: str = \"val_acc\",\n",
        "    restore_best_weights: bool = True,\n",
        "    use_cutmix: bool = True,\n",
        "    use_mixup: bool = True,\n",
        "    cutmix_prob: float = 0.5,\n",
        "    mixup_prob: float = 0.5,\n",
        "    cutmix_alpha: float = 1.0,\n",
        "    mixup_alpha: float = 0.2,\n",
        "    warmup_epochs: int = 5,\n",
        "):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    model = model.to(device)\n",
        "    count_parameters(model)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
        "\n",
        "    total_steps = len(train_loader) * num_epochs\n",
        "    warmup_steps = int(0.05 * total_steps)\n",
        "    scheduler = SequentialLR(\n",
        "        optimizer,\n",
        "        schedulers=[\n",
        "            LinearLR(optimizer, start_factor=0.1, total_iters=warmup_steps),\n",
        "            CosineAnnealingLR(optimizer, T_max=total_steps - warmup_steps, eta_min=1e-6),\n",
        "        ],\n",
        "        milestones=[warmup_steps],\n",
        "    )\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    scaler = GradScaler(enabled=(device.type != \"cpu\"))\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_ckpt = None\n",
        "    best_weights = None\n",
        "    early_stop_counter = 0\n",
        "\n",
        "    print_start(f\"ÌïôÏäµ Î†àÏ∏†Í≥†~ (CutMix: {use_cutmix}, MixUp: {use_mixup})\\n\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss, train_correct, total = 0.0, 0, 0\n",
        "\n",
        "        use_aug = epoch >= warmup_epochs\n",
        "\n",
        "        for batch_idx, (features, scalars, labels) in enumerate(train_loader):\n",
        "            features, scalars, labels = map(lambda x: x.to(device, non_blocking=True), (features, scalars, labels))\n",
        "            # non_blocking TrueÏì∞Î©¥ Í∞úÎπ†Î¶ÖÎãàÎã§\n",
        "\n",
        "            original_labels = labels.clone()\n",
        "            mixed = False\n",
        "\n",
        "            if use_aug and (use_cutmix or use_mixup):\n",
        "                r = np.random.rand()\n",
        "\n",
        "                if use_cutmix and r < cutmix_prob:\n",
        "                    features, labels, _, lam = cutmix_data(features, labels, cutmix_alpha, device.type)\n",
        "                    mixed = True\n",
        "                elif use_mixup and r < (cutmix_prob + mixup_prob):\n",
        "                    indices = torch.randperm(features.size(0)).to(device)\n",
        "                    lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
        "\n",
        "                    features = lam * features + (1 - lam) * features[indices]\n",
        "                    scalars = lam * scalars + (1 - lam) * scalars[indices]\n",
        "                    labels = lam * labels + (1 - lam) * labels[indices]\n",
        "                    mixed = True\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with autocast(enabled=(device.type != \"cpu\")):\n",
        "                logits = model(features, scalars)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "            if not mixed:\n",
        "                preds = (logits > 0.0).float()\n",
        "                train_correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    preds = (logits > 0.0).float()\n",
        "                    train_correct += (preds == original_labels).sum().item()\n",
        "                    total += original_labels.size(0)\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_acc = train_correct / total\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for features, scalars, labels in val_loader:\n",
        "                features, scalars, labels = map(lambda x: x.to(device, non_blocking=True), (features, scalars, labels))\n",
        "                with autocast(enabled=(device.type != \"cpu\")):\n",
        "                    logits = model(features, scalars)\n",
        "                    loss = criterion(logits, labels)\n",
        "\n",
        "                preds = (logits > 0.0).float()\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_acc = val_correct / val_total\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        aug_status = f\" [Aug: {'ON' if use_aug else 'OFF'}]\" if use_cutmix or use_mixup else \"\"\n",
        "        print(\n",
        "            f\"[Epoch {epoch+1:02d}]{aug_status} \"\n",
        "            f\"Train Loss: {avg_train_loss:.6} | Train Acc: {train_acc:.6f} || \"\n",
        "            f\"Val Loss: {avg_val_loss:.6f} | Val Acc: {val_acc:.6f}\"\n",
        "        )\n",
        "\n",
        "        if monitor == \"val_acc\":\n",
        "            metric = val_acc\n",
        "            best_metric = best_val_acc\n",
        "        else:\n",
        "            metric = -avg_val_loss\n",
        "            best_metric = -best_val_loss\n",
        "\n",
        "        if metric - best_metric > min_delta:\n",
        "            best_val_acc = val_acc\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_ckpt = os.path.join(save_dir, f\"best_epoch{epoch+1:02d}.pth\")\n",
        "            best_weights = model.state_dict() if restore_best_weights else None\n",
        "            early_stop_counter = 0\n",
        "            torch.save({\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"scheduler_state_dict\": scheduler.state_dict(),\n",
        "                \"val_acc\": val_acc,\n",
        "                \"val_loss\": avg_val_loss,\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"cutmix_used\": use_cutmix,\n",
        "                \"mixup_used\": use_mixup\n",
        "            }, best_ckpt)\n",
        "        else:\n",
        "            early_stop_counter += 1\n",
        "            if early_stop_counter >= patience:\n",
        "                print_warning(\"ÎÅù;;;;\")\n",
        "                if restore_best_weights and best_weights is not None:\n",
        "                    model.load_state_dict(best_weights)\n",
        "                break\n",
        "\n",
        "    return best_ckpt, best_val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFGQhzldT-vV"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-6gzUsDxDx1"
      },
      "outputs": [],
      "source": [
        "def load_model(ckpt_path: str, arch: str, num_scalar_features: int, device: torch.device):\n",
        "    if arch == 'vgg':\n",
        "        model = VGG(num_scalar_features=num_scalar_features).to(device)\n",
        "    elif arch == 'cnn8':\n",
        "        model = CNN8(num_scalar_features=num_scalar_features).to(device)\n",
        "    else:\n",
        "        raise ValueError(f\"Ïù¥Í≤åÎ®∏ÏûÑ? {arch}\")\n",
        "\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "    model.load_state_dict(ckpt['model_state_dict'] if 'model_state_dict' in ckpt else ckpt)\n",
        "    model.eval()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMuDnMS-T_sa"
      },
      "source": [
        "## Ensemble Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFmWSm_ccrbg"
      },
      "outputs": [],
      "source": [
        "def average_ensemble(\n",
        "    ckpt_paths: list[str],\n",
        "    archs:      list[str],\n",
        "    test_loader: DataLoader,\n",
        "    device:      torch.device,\n",
        "    num_scalar_features: int,\n",
        "):\n",
        "    assert len(ckpt_paths) == len(archs), \"Ï†úÎåÄÎ°ú ÌïòÏûê.\"\n",
        "\n",
        "    models = []\n",
        "    for idx, (path, arch) in enumerate(zip(ckpt_paths, archs), 1):\n",
        "        m = load_model(path, arch, num_scalar_features=num_scalar_features, device=device)\n",
        "        models.append(m)\n",
        "        print_info(f\"Î™®Îç∏: {idx}/{len(ckpt_paths)} @ '{path}'\")\n",
        "\n",
        "    all_ids = []\n",
        "    per_batch_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (feats, scals, ids) in enumerate(test_loader, 1):\n",
        "            feats = feats.to(device, non_blocking=True)\n",
        "            scals = scals.to(device, non_blocking=True)\n",
        "\n",
        "            logits_stack = torch.stack([m(feats, scals).view(-1) for m in models], dim=0)\n",
        "            probs = torch.sigmoid(logits_stack)\n",
        "            avg_probs = probs.mean(dim=0)\n",
        "\n",
        "            per_batch_probs.append(avg_probs.cpu().numpy())\n",
        "            all_ids.extend(ids)\n",
        "\n",
        "    final_probs = np.concatenate(per_batch_probs, axis=0)\n",
        "    print_success(\"ÏôÑÎ£å~\")\n",
        "    return all_ids, final_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkLmDAUezG0H"
      },
      "outputs": [],
      "source": [
        "def weighted_ensemble(\n",
        "    ckpt_paths: list[str],\n",
        "    archs: list[str],\n",
        "    test_loader: torch.utils.data.DataLoader,\n",
        "    device: torch.device,\n",
        "    num_scalar_features: int,\n",
        "    val_scores: list[float],\n",
        "    use_softmax_weights: bool = True\n",
        "):\n",
        "    assert len(ckpt_paths) == len(archs) == len(val_scores), \"Ï†úÎåÄÎ°úÌïòÏûê\"\n",
        "\n",
        "    weights = torch.tensor(val_scores, dtype=torch.float32)\n",
        "    weights = torch.softmax(weights, dim=0) if use_softmax_weights else weights / weights.sum()\n",
        "\n",
        "    models = []\n",
        "    for idx, (path, arch) in enumerate(zip(ckpt_paths, archs), 1):\n",
        "        model = load_model(path, arch, num_scalar_features=num_scalar_features, device=device)\n",
        "        models.append(model)\n",
        "\n",
        "    all_ids = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for feats, scalars, ids in test_loader:\n",
        "            feats, scalars = feats.to(device), scalars.to(device)\n",
        "\n",
        "            logits_stack = torch.stack([model(feats, scalars).view(-1) for model in models])\n",
        "            probs_stack = torch.sigmoid(logits_stack)\n",
        "\n",
        "            weighted_avg = (weights[:, None].to(device) * probs_stack).sum(dim=0)\n",
        "            all_probs.append(weighted_avg.cpu().numpy())\n",
        "            all_ids.extend(ids)\n",
        "\n",
        "    return all_ids, np.concatenate(all_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSzO74Y_w1Fv"
      },
      "source": [
        "## DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCvpXFGkw4iF"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(train_csv_path)\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "\n",
        "train_df_split, val_df_split = train_test_split(train_df, test_size=0.20, shuffle=True, random_state=42)\n",
        "\n",
        "train_dataset = DS(data_frame=train_df_split, feature_dir=precomputed_dir, is_training=True)\n",
        "val_dataset = DS(data_frame=val_df_split, feature_dir=precomputed_dir, is_training=True)\n",
        "test_dataset = DS(data_frame=test_df, feature_dir=precomputed_dir, is_training=False)\n",
        "\n",
        "batch_size = 512\n",
        "num_workers = 8\n",
        "prefetch = 2\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True,\n",
        "    prefetch_factor=prefetch,\n",
        "    collate_fn=collate_fn,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size * 2,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True,\n",
        "    prefetch_factor=prefetch,\n",
        "    collate_fn=collate_fn,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size * 2,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True,\n",
        "    prefetch_factor=prefetch,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmkAb7hNw7FA"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pm9G7VbMGO1"
      },
      "outputs": [],
      "source": [
        "num_scalars = 36\n",
        "epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8aEKh2bK1gP"
      },
      "outputs": [],
      "source": [
        "cnn8_model = CNN8(num_scalar_features=num_scalars)\n",
        "cnn8_ckpt, cnn8_val_acc = train_model(\n",
        "    model      = cnn8_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader  = val_loader,\n",
        "    device      = device,\n",
        "    save_dir    = \"./checkpoints/cnn8\",\n",
        "    num_epochs  = epochs,\n",
        "    base_lr=4e-4,\n",
        "    weight_decay=1e-4,\n",
        "    use_cutmix=True,\n",
        "    use_mixup=True,\n",
        "    cutmix_prob=0.6,\n",
        "    mixup_prob=0.4,\n",
        "    patience=25,\n",
        "    warmup_epochs=4,\n",
        ")\n",
        "print(f\"CNN8 best‚Äêval‚Äêacc = {cnn8_val_acc:.4f}, saved to {cnn8_ckpt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiJl_PHAoVat"
      },
      "outputs": [],
      "source": [
        "vgg_model = VGG(num_scalar_features=num_scalars)\n",
        "vgg_ckpt, vgg_val_acc = train_model(\n",
        "    model      = vgg_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader  = val_loader,\n",
        "    device      = device,\n",
        "    save_dir    = \"./checkpoints/vgg\",\n",
        "    num_epochs  = 140,\n",
        "    patience=55,\n",
        ")\n",
        "print(f\"VGG best‚Äêval‚Äêacc = {vgg_val_acc:.4f}, saved to {vgg_ckpt}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmkSzaMGxw8O"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIANmX1pxx8u"
      },
      "outputs": [],
      "source": [
        "# ckpt_paths = [cnn8_ckpt, resnet_ckpt, convgru_ckpt, vgg_ckpt, vgg_large_ckpt, convnext_ckpt, resnet_se_stochdepth_ckpt]\n",
        "# raw_scores = [cnn8_val_acc, resnet_val_acc, convgru_val_acc, vgg_val_acc, vgg_large_val_acc, convnext_val_acc, resnet_se_stochdepth_val_acc]\n",
        "# archs = [\"cnn8\", \"resnet\", \"convgru\", \"vgg\", \"vgg_large\", \"convnext\", \"resnet_se_stochdepth\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_paths = [cnn8_ckpt, vgg_ckpt]\n",
        "raw_scores = [cnn8_val_acc, vgg_val_acc]\n",
        "archs = [\"cnn8\", \"vgg\"]"
      ],
      "metadata": {
        "id": "X1e9ggLJgIGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozZja_24odyC"
      },
      "outputs": [],
      "source": [
        "all_ids, avg_probs = weighted_ensemble(\n",
        "    ckpt_paths=ckpt_paths,\n",
        "    archs=archs,\n",
        "    test_loader=test_loader,\n",
        "    device=device,\n",
        "    num_scalar_features=num_scalars,\n",
        "    val_scores=raw_scores\n",
        ")\n",
        "\n",
        "predictions = (avg_probs > 0.5).astype(int)\n",
        "final_labels = [\"E\" if p == 1 else \"I\" for p in predictions]\n",
        "\n",
        "submission_df = pd.DataFrame({\n",
        "    \"ID\": all_ids,\n",
        "    \"Target\": final_labels\n",
        "})\n",
        "submission_df.to_csv(\"./ensemble_submission.csv\", index=False)\n",
        "print(submission_df.head(10))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WToQ68s2hDpG",
        "o8nNA20Gx6mN",
        "AzJkk3EtT4_U",
        "duEY6nGUT8C_",
        "rMuDnMS-T_sa"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}